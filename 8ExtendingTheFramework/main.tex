\chapter{
Extending the framework
(V0.0)
}



\draftnote{blue}{Stochastic worlds}{
Keep transformations deterministic, but actions 'select' different transformations based on a probability distribution.
}

\draftnote{blue}{Continuous worlds}{
In the continuous case, atomic transformations are infinitesimal transformations.
}

draftnote{red}{Tracelet theory - Nicolas Behr}{
	Extracting algebras from graphs.
}

\draftnote{blue}{To do: non-ideal sensors}{
Section on non-ideal sensors and how we need to adjust the idea of what is required for the decision process of the agent to be a Markov process.
For example, in the non-ideal sensors case the current representation state needs to be included in what the agent uses to make its decision since occlusion (i.e., $b(w_{1})=b(w_{2})$) is possible (thought experiment where there is a wall and either an object is hidden behind the wall or there is no object behind the wall; in this case, the agent `knowing` that the object is hidden behind the wall would affect the agent's prediction of what its observation state would be when it looks behind the wall.
)
}

\draftnote{blue}{More realtistic inference mapping}{
We'd actually have
\begin{equation}
	h: O \times Z \to Z,
\end{equation}
such that
\begin{equation}
	h(o_{t+1}, z_{t}) = z_{t+1}
\end{equation}
where $o_{t+1}$ is the observation state at time $t+1$, $z_{t}$ is the representation state at time $t$, and $z_{t+1}$ is the representaiton state at time $t+1$.
This is because the next representation state should depend on the current representation state.
Do we have something where the representation of the action is affected by the change from the old to the new observation states ?
So somehting like:
\begin{equation}
	h: A \times O \to Z
\end{equation}
where $A$ is the representation fo the action ?
}
